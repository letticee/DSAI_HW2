{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 80000\n",
    "DIGITS = 3\n",
    "REVERSE = False\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "chars = '0123456789+- '\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "    \n",
    "    def encode(self, C, num_rows):\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return \"\".join(self.indices_char[i] for i in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctable = CharacterTable(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ' ',\n",
       " 1: '+',\n",
       " 2: '-',\n",
       " 3: '0',\n",
       " 4: '1',\n",
       " 5: '2',\n",
       " 6: '3',\n",
       " 7: '4',\n",
       " 8: '5',\n",
       " 9: '6',\n",
       " 10: '7',\n",
       " 11: '8',\n",
       " 12: '9'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctable.indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 80000\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    op = np.random.randint(2)\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, DIGITS + 1))))\n",
    "    if op == 0:\n",
    "        a, b = f(), f()\n",
    "        q = '{}+{}'.format(a, b)\n",
    "        ans = str(a + b)    \n",
    "    else:\n",
    "        num = [f(),f()]\n",
    "        num.sort()\n",
    "        a = num[1]\n",
    "        b = num[0]\n",
    "        q = '{}-{}'.format(a, b)\n",
    "        ans = str(a - b)\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    if REVERSE:\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['796+83 ', '97-7   ', '5+779  ', '78-6   ', '98-1   '] ['879 ', '90  ', '784 ', '72  ', '97  ']\n"
     ]
    }
   ],
   "source": [
    "print(questions[:5], expected[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(expected), DIGITS + 1, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:\n",
      "(18000, 7, 13)\n",
      "(18000, 4, 13)\n",
      "Validation Data:\n",
      "(2000, 7, 13)\n",
      "(2000, 4, 13)\n",
      "Testing Data:\n",
      "(60000, 7, 13)\n",
      "(60000, 4, 13)\n"
     ]
    }
   ],
   "source": [
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# train_test_split\n",
    "train_x = x[:20000]\n",
    "train_y = y[:20000]\n",
    "test_x = x[20000:]\n",
    "test_y = y[20000:]\n",
    "\n",
    "split_at = len(train_x) - len(train_x) // 10\n",
    "(x_train, x_val) = train_x[:split_at], train_x[split_at:]\n",
    "(y_train, y_val) = train_y[:split_at], train_y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "\n",
    "print('Testing Data:')\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  [[[False False False False  True False False False False False False\n",
      "   False False]\n",
      "  [False False False False False False False False False False False\n",
      "    True False]\n",
      "  [False False False False False False False False False  True False\n",
      "   False False]\n",
      "  [False False  True False False False False False False False False\n",
      "   False False]\n",
      "  [False False False False False False False  True False False False\n",
      "   False False]\n",
      "  [False False False False False False False False False False False\n",
      "   False  True]\n",
      "  [ True False False False False False False False False False False\n",
      "   False False]]\n",
      "\n",
      " [[False False False False False False False False False  True False\n",
      "   False False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [False False False False False False False False False False False\n",
      "    True False]\n",
      "  [False False  True False False False False False False False False\n",
      "   False False]\n",
      "  [False False False False  True False False False False False False\n",
      "   False False]\n",
      "  [False False False  True False False False False False False False\n",
      "   False False]\n",
      "  [False False False False False False False False  True False False\n",
      "   False False]]\n",
      "\n",
      " [[False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [False False False False False False False False False False False\n",
      "    True False]\n",
      "  [False False False False False False False False False False False\n",
      "   False  True]\n",
      "  [False False  True False False False False False False False False\n",
      "   False False]\n",
      "  [False False False False  True False False False False False False\n",
      "   False False]\n",
      "  [False False False False False False  True False False False False\n",
      "   False False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False False]]] \n",
      "\n",
      " label:  [[[False False False False  True False False False False False False\n",
      "   False False]\n",
      "  [False False False False False False  True False False False False\n",
      "   False False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False False]]\n",
      "\n",
      " [[False False False False False False False False  True False False\n",
      "   False False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [False False False False False False  True False False False False\n",
      "   False False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False False]]\n",
      "\n",
      " [[False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [False False False False False False False False False False  True\n",
      "   False False]\n",
      "  [False False False False False False False False False  True False\n",
      "   False False]\n",
      "  [ True False False False False False False False False False False\n",
      "   False False]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input: \", x_train[:3], '\\n\\n', \"label: \", y_train[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Users\\SCREAM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72704     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 13)             1677      \n",
      "=================================================================\n",
      "Total params: 205,965\n",
      "Trainable params: 205,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "from keras.layers import Dense\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "\n",
    "############################################\n",
    "##### Build your own model here ############\n",
    "############################################\n",
    "n_chars = len(chars)\n",
    "# create LSTM\n",
    "model = Sequential()\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
    "model.add(layers.RepeatVector(DIGITS + 1))\n",
    "for _ in range(LAYERS):\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SCREAM\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "18000/18000 [==============================] - 8s 431us/step - loss: 1.9834 - acc: 0.3042 - val_loss: 1.8264 - val_acc: 0.3396\n",
      "Epoch 2/200\n",
      "18000/18000 [==============================] - 4s 197us/step - loss: 1.8070 - acc: 0.3365 - val_loss: 1.7973 - val_acc: 0.3354\n",
      "Epoch 3/200\n",
      "18000/18000 [==============================] - 3s 191us/step - loss: 1.7907 - acc: 0.3384 - val_loss: 1.7748 - val_acc: 0.3376\n",
      "Epoch 4/200\n",
      "18000/18000 [==============================] - 3s 186us/step - loss: 1.7656 - acc: 0.3428 - val_loss: 1.7628 - val_acc: 0.3444\n",
      "Epoch 5/200\n",
      "18000/18000 [==============================] - 3s 190us/step - loss: 1.7295 - acc: 0.3543 - val_loss: 1.7170 - val_acc: 0.3650\n",
      "Epoch 6/200\n",
      "18000/18000 [==============================] - 3s 183us/step - loss: 1.6891 - acc: 0.3728 - val_loss: 1.6827 - val_acc: 0.3723\n",
      "Epoch 7/200\n",
      "18000/18000 [==============================] - 3s 183us/step - loss: 1.6484 - acc: 0.3874 - val_loss: 1.6435 - val_acc: 0.3861\n",
      "Epoch 8/200\n",
      "18000/18000 [==============================] - 3s 183us/step - loss: 1.6132 - acc: 0.3999 - val_loss: 1.5996 - val_acc: 0.4065\n",
      "Epoch 9/200\n",
      "18000/18000 [==============================] - 3s 186us/step - loss: 1.5745 - acc: 0.4139 - val_loss: 1.5638 - val_acc: 0.4166\n",
      "Epoch 10/200\n",
      "18000/18000 [==============================] - 6s 307us/step - loss: 1.5461 - acc: 0.4227 - val_loss: 1.5338 - val_acc: 0.4270\n",
      "Epoch 11/200\n",
      "18000/18000 [==============================] - 4s 225us/step - loss: 1.5125 - acc: 0.4364 - val_loss: 1.5145 - val_acc: 0.4370\n",
      "Epoch 12/200\n",
      "18000/18000 [==============================] - 3s 188us/step - loss: 1.4847 - acc: 0.4460 - val_loss: 1.4798 - val_acc: 0.4403\n",
      "Epoch 13/200\n",
      "18000/18000 [==============================] - 3s 191us/step - loss: 1.4508 - acc: 0.4598 - val_loss: 1.4529 - val_acc: 0.4536\n",
      "Epoch 14/200\n",
      "18000/18000 [==============================] - 4s 198us/step - loss: 1.4217 - acc: 0.4689 - val_loss: 1.4207 - val_acc: 0.4731\n",
      "Epoch 15/200\n",
      "18000/18000 [==============================] - 4s 198us/step - loss: 1.3875 - acc: 0.4859 - val_loss: 1.3847 - val_acc: 0.4839\n",
      "Epoch 16/200\n",
      "18000/18000 [==============================] - 4s 197us/step - loss: 1.3606 - acc: 0.4953 - val_loss: 1.3779 - val_acc: 0.4821\n",
      "Epoch 17/200\n",
      "18000/18000 [==============================] - 4s 203us/step - loss: 1.3343 - acc: 0.5048 - val_loss: 1.3377 - val_acc: 0.4971\n",
      "Epoch 18/200\n",
      "18000/18000 [==============================] - 4s 223us/step - loss: 1.3079 - acc: 0.5180 - val_loss: 1.3137 - val_acc: 0.5036\n",
      "Epoch 19/200\n",
      "18000/18000 [==============================] - 4s 215us/step - loss: 1.2881 - acc: 0.5236 - val_loss: 1.2916 - val_acc: 0.5136\n",
      "Epoch 20/200\n",
      "18000/18000 [==============================] - 3s 165us/step - loss: 1.2679 - acc: 0.5316 - val_loss: 1.2762 - val_acc: 0.5228\n",
      "Epoch 21/200\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 1.2505 - acc: 0.5378 - val_loss: 1.2577 - val_acc: 0.5297\n",
      "Epoch 22/200\n",
      "18000/18000 [==============================] - 4s 199us/step - loss: 1.2360 - acc: 0.5439 - val_loss: 1.2442 - val_acc: 0.5326\n",
      "Epoch 23/200\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 1.2074 - acc: 0.5553 - val_loss: 1.2305 - val_acc: 0.5420\n",
      "Epoch 24/200\n",
      "18000/18000 [==============================] - 2s 138us/step - loss: 1.1937 - acc: 0.5600 - val_loss: 1.2146 - val_acc: 0.5434\n",
      "Epoch 25/200\n",
      "18000/18000 [==============================] - 3s 155us/step - loss: 1.1783 - acc: 0.5660 - val_loss: 1.1992 - val_acc: 0.5486\n",
      "Epoch 26/200\n",
      "18000/18000 [==============================] - 2s 137us/step - loss: 1.1608 - acc: 0.5714 - val_loss: 1.1745 - val_acc: 0.5610\n",
      "Epoch 27/200\n",
      "18000/18000 [==============================] - 3s 146us/step - loss: 1.1383 - acc: 0.5782 - val_loss: 1.1589 - val_acc: 0.5636\n",
      "Epoch 28/200\n",
      "18000/18000 [==============================] - 3s 152us/step - loss: 1.1258 - acc: 0.5813 - val_loss: 1.1753 - val_acc: 0.5575\n",
      "Epoch 29/200\n",
      "18000/18000 [==============================] - 3s 151us/step - loss: 1.1092 - acc: 0.5912 - val_loss: 1.1368 - val_acc: 0.5747\n",
      "Epoch 30/200\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 1.0913 - acc: 0.5953 - val_loss: 1.1175 - val_acc: 0.5811\n",
      "Epoch 31/200\n",
      "18000/18000 [==============================] - 2s 137us/step - loss: 1.0706 - acc: 0.6045 - val_loss: 1.1020 - val_acc: 0.5900\n",
      "Epoch 32/200\n",
      "18000/18000 [==============================] - 2s 136us/step - loss: 1.0540 - acc: 0.6093 - val_loss: 1.0980 - val_acc: 0.5812\n",
      "Epoch 33/200\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 1.0405 - acc: 0.6156 - val_loss: 1.0791 - val_acc: 0.5932\n",
      "Epoch 34/200\n",
      "18000/18000 [==============================] - 2s 135us/step - loss: 1.0201 - acc: 0.6229 - val_loss: 1.0699 - val_acc: 0.5940\n",
      "Epoch 35/200\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 1.0083 - acc: 0.6268 - val_loss: 1.0523 - val_acc: 0.6031\n",
      "Epoch 36/200\n",
      "18000/18000 [==============================] - 3s 140us/step - loss: 0.9930 - acc: 0.6317 - val_loss: 1.0452 - val_acc: 0.6049\n",
      "Epoch 37/200\n",
      "18000/18000 [==============================] - 3s 145us/step - loss: 0.9792 - acc: 0.6397 - val_loss: 1.0318 - val_acc: 0.6114\n",
      "Epoch 38/200\n",
      "18000/18000 [==============================] - 3s 184us/step - loss: 0.9623 - acc: 0.6459 - val_loss: 1.0197 - val_acc: 0.6144\n",
      "Epoch 39/200\n",
      "18000/18000 [==============================] - 3s 153us/step - loss: 0.9507 - acc: 0.6495 - val_loss: 1.0203 - val_acc: 0.6166\n",
      "Epoch 40/200\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 0.9401 - acc: 0.6541 - val_loss: 1.0164 - val_acc: 0.6174\n",
      "Epoch 41/200\n",
      "18000/18000 [==============================] - 3s 157us/step - loss: 0.9239 - acc: 0.6592 - val_loss: 0.9976 - val_acc: 0.6176\n",
      "Epoch 42/200\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 0.9125 - acc: 0.6646 - val_loss: 0.9909 - val_acc: 0.6246\n",
      "Epoch 43/200\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 0.8958 - acc: 0.6720 - val_loss: 0.9756 - val_acc: 0.6320\n",
      "Epoch 44/200\n",
      "18000/18000 [==============================] - 3s 147us/step - loss: 0.8877 - acc: 0.6751 - val_loss: 0.9714 - val_acc: 0.6319\n",
      "Epoch 45/200\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 0.8756 - acc: 0.6797 - val_loss: 0.9580 - val_acc: 0.6325\n",
      "Epoch 46/200\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 0.8567 - acc: 0.6878 - val_loss: 0.9560 - val_acc: 0.6340\n",
      "Epoch 47/200\n",
      "18000/18000 [==============================] - 3s 155us/step - loss: 0.8503 - acc: 0.6888 - val_loss: 0.9432 - val_acc: 0.6421\n",
      "Epoch 48/200\n",
      "18000/18000 [==============================] - 3s 148us/step - loss: 0.8373 - acc: 0.6953 - val_loss: 0.9375 - val_acc: 0.6426\n",
      "Epoch 49/200\n",
      "18000/18000 [==============================] - 3s 149us/step - loss: 0.8248 - acc: 0.7004 - val_loss: 0.9346 - val_acc: 0.6450\n",
      "Epoch 50/200\n",
      "18000/18000 [==============================] - ETA: 0s - loss: 0.8126 - acc: 0.704 - 3s 149us/step - loss: 0.8127 - acc: 0.7044 - val_loss: 0.9260 - val_acc: 0.6483\n",
      "Epoch 51/200\n",
      "18000/18000 [==============================] - 3s 161us/step - loss: 0.7997 - acc: 0.7091 - val_loss: 0.9320 - val_acc: 0.6474\n",
      "Epoch 52/200\n",
      "18000/18000 [==============================] - 3s 153us/step - loss: 0.7881 - acc: 0.7150 - val_loss: 0.9098 - val_acc: 0.6549\n",
      "Epoch 53/200\n",
      "18000/18000 [==============================] - 3s 156us/step - loss: 0.7772 - acc: 0.7189 - val_loss: 0.9023 - val_acc: 0.6574\n",
      "Epoch 54/200\n",
      "18000/18000 [==============================] - 3s 142us/step - loss: 0.7664 - acc: 0.7233 - val_loss: 0.9112 - val_acc: 0.6570\n",
      "Epoch 55/200\n",
      "18000/18000 [==============================] - 3s 165us/step - loss: 0.7563 - acc: 0.7264 - val_loss: 0.8925 - val_acc: 0.6574\n",
      "Epoch 56/200\n",
      "18000/18000 [==============================] - 3s 151us/step - loss: 0.7428 - acc: 0.7316 - val_loss: 0.8813 - val_acc: 0.6635\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 3s 180us/step - loss: 0.7295 - acc: 0.7372 - val_loss: 0.8692 - val_acc: 0.6718\n",
      "Epoch 58/200\n",
      "18000/18000 [==============================] - 3s 183us/step - loss: 0.7188 - acc: 0.7412 - val_loss: 0.8771 - val_acc: 0.6665\n",
      "Epoch 59/200\n",
      "18000/18000 [==============================] - 3s 168us/step - loss: 0.7083 - acc: 0.7445 - val_loss: 0.8695 - val_acc: 0.6714\n",
      "Epoch 60/200\n",
      "18000/18000 [==============================] - 3s 172us/step - loss: 0.7003 - acc: 0.7468 - val_loss: 0.8707 - val_acc: 0.6672\n",
      "Epoch 61/200\n",
      "18000/18000 [==============================] - 3s 178us/step - loss: 0.6818 - acc: 0.7554 - val_loss: 0.8454 - val_acc: 0.6780\n",
      "Epoch 62/200\n",
      "18000/18000 [==============================] - 3s 163us/step - loss: 0.6720 - acc: 0.7584 - val_loss: 0.8459 - val_acc: 0.6763\n",
      "Epoch 63/200\n",
      "18000/18000 [==============================] - 3s 162us/step - loss: 0.6573 - acc: 0.7634 - val_loss: 0.8429 - val_acc: 0.6782\n",
      "Epoch 64/200\n",
      "18000/18000 [==============================] - 3s 146us/step - loss: 0.6390 - acc: 0.7732 - val_loss: 0.8212 - val_acc: 0.6806\n",
      "Epoch 65/200\n",
      "18000/18000 [==============================] - 3s 152us/step - loss: 0.6313 - acc: 0.7732 - val_loss: 0.8248 - val_acc: 0.6852\n",
      "Epoch 66/200\n",
      "18000/18000 [==============================] - 3s 168us/step - loss: 0.6130 - acc: 0.7811 - val_loss: 0.8058 - val_acc: 0.6930\n",
      "Epoch 67/200\n",
      "18000/18000 [==============================] - 3s 184us/step - loss: 0.6015 - acc: 0.7848 - val_loss: 0.7987 - val_acc: 0.6946\n",
      "Epoch 68/200\n",
      "18000/18000 [==============================] - 3s 170us/step - loss: 0.5844 - acc: 0.7912 - val_loss: 0.7762 - val_acc: 0.6974\n",
      "Epoch 69/200\n",
      "18000/18000 [==============================] - 5s 254us/step - loss: 0.5679 - acc: 0.7987 - val_loss: 0.7793 - val_acc: 0.6946\n",
      "Epoch 70/200\n",
      "18000/18000 [==============================] - 5s 261us/step - loss: 0.5616 - acc: 0.7992 - val_loss: 0.7642 - val_acc: 0.7012\n",
      "Epoch 71/200\n",
      "18000/18000 [==============================] - 5s 277us/step - loss: 0.5382 - acc: 0.8091 - val_loss: 0.7458 - val_acc: 0.7109\n",
      "Epoch 72/200\n",
      "18000/18000 [==============================] - 5s 282us/step - loss: 0.5236 - acc: 0.8141 - val_loss: 0.7316 - val_acc: 0.7134\n",
      "Epoch 73/200\n",
      "18000/18000 [==============================] - 5s 275us/step - loss: 0.5074 - acc: 0.8202 - val_loss: 0.7230 - val_acc: 0.7160\n",
      "Epoch 74/200\n",
      "18000/18000 [==============================] - 5s 278us/step - loss: 0.4921 - acc: 0.8259 - val_loss: 0.7053 - val_acc: 0.7231\n",
      "Epoch 75/200\n",
      "18000/18000 [==============================] - 5s 283us/step - loss: 0.4779 - acc: 0.8318 - val_loss: 0.7012 - val_acc: 0.7239\n",
      "Epoch 76/200\n",
      "18000/18000 [==============================] - 5s 277us/step - loss: 0.4625 - acc: 0.8372 - val_loss: 0.6925 - val_acc: 0.7249\n",
      "Epoch 77/200\n",
      "18000/18000 [==============================] - 5s 276us/step - loss: 0.4488 - acc: 0.8438 - val_loss: 0.6746 - val_acc: 0.7358\n",
      "Epoch 78/200\n",
      "18000/18000 [==============================] - 5s 289us/step - loss: 0.4276 - acc: 0.8538 - val_loss: 0.6583 - val_acc: 0.7397\n",
      "Epoch 79/200\n",
      "18000/18000 [==============================] - 5s 280us/step - loss: 0.4164 - acc: 0.8567 - val_loss: 0.6398 - val_acc: 0.7474\n",
      "Epoch 80/200\n",
      "18000/18000 [==============================] - 5s 290us/step - loss: 0.4032 - acc: 0.8642 - val_loss: 0.6301 - val_acc: 0.7448\n",
      "Epoch 81/200\n",
      "18000/18000 [==============================] - 5s 279us/step - loss: 0.3856 - acc: 0.8709 - val_loss: 0.6062 - val_acc: 0.7626\n",
      "Epoch 82/200\n",
      "18000/18000 [==============================] - 4s 250us/step - loss: 0.3700 - acc: 0.8783 - val_loss: 0.6030 - val_acc: 0.7566\n",
      "Epoch 83/200\n",
      "18000/18000 [==============================] - 4s 246us/step - loss: 0.3584 - acc: 0.8820 - val_loss: 0.5954 - val_acc: 0.7636\n",
      "Epoch 84/200\n",
      "18000/18000 [==============================] - 4s 246us/step - loss: 0.3452 - acc: 0.8884 - val_loss: 0.5900 - val_acc: 0.7654\n",
      "Epoch 85/200\n",
      "18000/18000 [==============================] - 5s 253us/step - loss: 0.3433 - acc: 0.8882 - val_loss: 0.5700 - val_acc: 0.7741\n",
      "Epoch 86/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.3220 - acc: 0.8991 - val_loss: 0.5549 - val_acc: 0.7754\n",
      "Epoch 87/200\n",
      "18000/18000 [==============================] - 4s 245us/step - loss: 0.3126 - acc: 0.9024 - val_loss: 0.5597 - val_acc: 0.7731\n",
      "Epoch 88/200\n",
      "18000/18000 [==============================] - 4s 248us/step - loss: 0.2992 - acc: 0.9088 - val_loss: 0.5428 - val_acc: 0.7868\n",
      "Epoch 89/200\n",
      "18000/18000 [==============================] - 5s 256us/step - loss: 0.2880 - acc: 0.9141 - val_loss: 0.5326 - val_acc: 0.7884\n",
      "Epoch 90/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.2789 - acc: 0.9176 - val_loss: 0.5211 - val_acc: 0.7971\n",
      "Epoch 91/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.2708 - acc: 0.9207 - val_loss: 0.5302 - val_acc: 0.7918\n",
      "Epoch 92/200\n",
      "18000/18000 [==============================] - 5s 257us/step - loss: 0.2732 - acc: 0.9166 - val_loss: 0.5066 - val_acc: 0.8001\n",
      "Epoch 93/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.2464 - acc: 0.9316 - val_loss: 0.4961 - val_acc: 0.8065\n",
      "Epoch 94/200\n",
      "18000/18000 [==============================] - 4s 248us/step - loss: 0.2381 - acc: 0.9358 - val_loss: 0.4936 - val_acc: 0.8084\n",
      "Epoch 95/200\n",
      "18000/18000 [==============================] - 5s 254us/step - loss: 0.2302 - acc: 0.9391 - val_loss: 0.4958 - val_acc: 0.8089\n",
      "Epoch 96/200\n",
      "18000/18000 [==============================] - 5s 286us/step - loss: 0.2308 - acc: 0.9371 - val_loss: 0.4885 - val_acc: 0.8071\n",
      "Epoch 97/200\n",
      "18000/18000 [==============================] - 5s 253us/step - loss: 0.2177 - acc: 0.9427 - val_loss: 0.4790 - val_acc: 0.8130\n",
      "Epoch 98/200\n",
      "18000/18000 [==============================] - 4s 250us/step - loss: 0.2036 - acc: 0.9495 - val_loss: 0.4645 - val_acc: 0.8224\n",
      "Epoch 99/200\n",
      "18000/18000 [==============================] - 5s 254us/step - loss: 0.1973 - acc: 0.9519 - val_loss: 0.5007 - val_acc: 0.8068\n",
      "Epoch 100/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.2086 - acc: 0.9438 - val_loss: 0.4811 - val_acc: 0.8165\n",
      "Epoch 101/200\n",
      "18000/18000 [==============================] - 5s 252us/step - loss: 0.1986 - acc: 0.9482 - val_loss: 0.4633 - val_acc: 0.8236\n",
      "Epoch 102/200\n",
      "18000/18000 [==============================] - 4s 249us/step - loss: 0.1799 - acc: 0.9582 - val_loss: 0.4542 - val_acc: 0.8284\n",
      "Epoch 103/200\n",
      "18000/18000 [==============================] - 5s 258us/step - loss: 0.1655 - acc: 0.9641 - val_loss: 0.4404 - val_acc: 0.8329\n",
      "Epoch 104/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.1578 - acc: 0.9676 - val_loss: 0.4407 - val_acc: 0.8347\n",
      "Epoch 105/200\n",
      "18000/18000 [==============================] - 4s 246us/step - loss: 0.1564 - acc: 0.9666 - val_loss: 0.4390 - val_acc: 0.8331\n",
      "Epoch 106/200\n",
      "18000/18000 [==============================] - 5s 256us/step - loss: 0.1475 - acc: 0.9705 - val_loss: 0.4443 - val_acc: 0.8304\n",
      "Epoch 107/200\n",
      "18000/18000 [==============================] - 5s 275us/step - loss: 0.1535 - acc: 0.9660 - val_loss: 0.4242 - val_acc: 0.8366\n",
      "Epoch 108/200\n",
      "18000/18000 [==============================] - 5s 284us/step - loss: 0.1460 - acc: 0.9689 - val_loss: 0.4330 - val_acc: 0.8394\n",
      "Epoch 109/200\n",
      "18000/18000 [==============================] - 5s 302us/step - loss: 0.1327 - acc: 0.9751 - val_loss: 0.4297 - val_acc: 0.8426\n",
      "Epoch 110/200\n",
      "18000/18000 [==============================] - 5s 266us/step - loss: 0.1491 - acc: 0.9649 - val_loss: 0.4729 - val_acc: 0.8191\n",
      "Epoch 111/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.1659 - acc: 0.9563 - val_loss: 0.4731 - val_acc: 0.8235\n",
      "Epoch 112/200\n",
      "18000/18000 [==============================] - 4s 247us/step - loss: 0.1460 - acc: 0.9656 - val_loss: 0.4218 - val_acc: 0.8389\n",
      "Epoch 113/200\n",
      "18000/18000 [==============================] - 5s 257us/step - loss: 0.1169 - acc: 0.9793 - val_loss: 0.4032 - val_acc: 0.8486\n",
      "Epoch 114/200\n",
      "18000/18000 [==============================] - 5s 287us/step - loss: 0.1058 - acc: 0.9840 - val_loss: 0.3996 - val_acc: 0.8515\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 5s 280us/step - loss: 0.0989 - acc: 0.9861 - val_loss: 0.3984 - val_acc: 0.8564\n",
      "Epoch 116/200\n",
      "18000/18000 [==============================] - 5s 289us/step - loss: 0.0950 - acc: 0.9866 - val_loss: 0.3950 - val_acc: 0.8505\n",
      "Epoch 117/200\n",
      "18000/18000 [==============================] - 5s 268us/step - loss: 0.0910 - acc: 0.9878 - val_loss: 0.4028 - val_acc: 0.8528\n",
      "Epoch 118/200\n",
      "18000/18000 [==============================] - 5s 268us/step - loss: 0.1069 - acc: 0.9796 - val_loss: 0.4359 - val_acc: 0.8427\n",
      "Epoch 119/200\n",
      "18000/18000 [==============================] - 5s 277us/step - loss: 0.1110 - acc: 0.9774 - val_loss: 0.3978 - val_acc: 0.8538\n",
      "Epoch 120/200\n",
      "18000/18000 [==============================] - 5s 282us/step - loss: 0.0838 - acc: 0.9894 - val_loss: 0.3925 - val_acc: 0.8555\n",
      "Epoch 121/200\n",
      "18000/18000 [==============================] - 4s 248us/step - loss: 0.0805 - acc: 0.9897 - val_loss: 0.4014 - val_acc: 0.8573\n",
      "Epoch 122/200\n",
      "18000/18000 [==============================] - 5s 257us/step - loss: 0.0789 - acc: 0.9900 - val_loss: 0.4020 - val_acc: 0.8574\n",
      "Epoch 123/200\n",
      "18000/18000 [==============================] - 5s 263us/step - loss: 0.0738 - acc: 0.9913 - val_loss: 0.3965 - val_acc: 0.8597\n",
      "Epoch 124/200\n",
      "18000/18000 [==============================] - 5s 255us/step - loss: 0.0755 - acc: 0.9897 - val_loss: 0.4074 - val_acc: 0.8550\n",
      "Epoch 125/200\n",
      "18000/18000 [==============================] - 5s 252us/step - loss: 0.0755 - acc: 0.9895 - val_loss: 0.4022 - val_acc: 0.8551\n",
      "Epoch 126/200\n",
      "18000/18000 [==============================] - 5s 262us/step - loss: 0.0733 - acc: 0.9897 - val_loss: 0.4036 - val_acc: 0.8534\n",
      "Epoch 127/200\n",
      "18000/18000 [==============================] - 4s 250us/step - loss: 0.0977 - acc: 0.9777 - val_loss: 0.4596 - val_acc: 0.8375\n",
      "Epoch 128/200\n",
      "18000/18000 [==============================] - 5s 256us/step - loss: 0.0922 - acc: 0.9800 - val_loss: 0.4215 - val_acc: 0.8505\n",
      "Epoch 129/200\n",
      "18000/18000 [==============================] - 5s 268us/step - loss: 0.0623 - acc: 0.9930 - val_loss: 0.3932 - val_acc: 0.8649\n",
      "Epoch 130/200\n",
      "18000/18000 [==============================] - 5s 276us/step - loss: 0.0529 - acc: 0.9958 - val_loss: 0.3860 - val_acc: 0.8709\n",
      "Epoch 131/200\n",
      "18000/18000 [==============================] - 5s 292us/step - loss: 0.0500 - acc: 0.9964 - val_loss: 0.3843 - val_acc: 0.8641\n",
      "Epoch 132/200\n",
      "18000/18000 [==============================] - 5s 258us/step - loss: 0.0497 - acc: 0.9959 - val_loss: 0.3835 - val_acc: 0.8666\n",
      "Epoch 133/200\n",
      "18000/18000 [==============================] - 5s 272us/step - loss: 0.0481 - acc: 0.9962 - val_loss: 0.3985 - val_acc: 0.8636\n",
      "Epoch 134/200\n",
      "18000/18000 [==============================] - 5s 258us/step - loss: 0.0501 - acc: 0.9947 - val_loss: 0.4339 - val_acc: 0.8519\n",
      "Epoch 135/200\n",
      "18000/18000 [==============================] - 5s 253us/step - loss: 0.1488 - acc: 0.9527 - val_loss: 0.5864 - val_acc: 0.8174\n",
      "Epoch 136/200\n",
      "18000/18000 [==============================] - 5s 262us/step - loss: 0.1079 - acc: 0.9693 - val_loss: 0.4159 - val_acc: 0.8625\n",
      "Epoch 137/200\n",
      "18000/18000 [==============================] - 5s 273us/step - loss: 0.0539 - acc: 0.9929 - val_loss: 0.3710 - val_acc: 0.8727\n",
      "Epoch 138/200\n",
      "18000/18000 [==============================] - 5s 286us/step - loss: 0.0391 - acc: 0.9977 - val_loss: 0.3691 - val_acc: 0.8756\n",
      "Epoch 139/200\n",
      "18000/18000 [==============================] - 6s 319us/step - loss: 0.0357 - acc: 0.9982 - val_loss: 0.3734 - val_acc: 0.8725\n",
      "Epoch 140/200\n",
      "18000/18000 [==============================] - 6s 320us/step - loss: 0.0340 - acc: 0.9983 - val_loss: 0.3709 - val_acc: 0.8755\n",
      "Epoch 141/200\n",
      "18000/18000 [==============================] - 5s 302us/step - loss: 0.0326 - acc: 0.9985 - val_loss: 0.3805 - val_acc: 0.8730\n",
      "Epoch 142/200\n",
      "18000/18000 [==============================] - 5s 269us/step - loss: 0.0317 - acc: 0.9985 - val_loss: 0.3797 - val_acc: 0.8735\n",
      "Epoch 143/200\n",
      "18000/18000 [==============================] - 5s 297us/step - loss: 0.0317 - acc: 0.9985 - val_loss: 0.3847 - val_acc: 0.8719\n",
      "Epoch 144/200\n",
      "18000/18000 [==============================] - 5s 291us/step - loss: 0.0302 - acc: 0.9987 - val_loss: 0.3870 - val_acc: 0.8730\n",
      "Epoch 145/200\n",
      "18000/18000 [==============================] - 5s 287us/step - loss: 0.0295 - acc: 0.9985 - val_loss: 0.3891 - val_acc: 0.8744\n",
      "Epoch 146/200\n",
      "18000/18000 [==============================] - 5s 280us/step - loss: 0.0293 - acc: 0.9988 - val_loss: 0.3871 - val_acc: 0.8755\n",
      "Epoch 147/200\n",
      "18000/18000 [==============================] - 5s 278us/step - loss: 0.0326 - acc: 0.9977 - val_loss: 0.4095 - val_acc: 0.8690\n",
      "Epoch 148/200\n",
      "18000/18000 [==============================] - 5s 288us/step - loss: 0.1337 - acc: 0.9558 - val_loss: 0.5416 - val_acc: 0.8379\n",
      "Epoch 149/200\n",
      "18000/18000 [==============================] - 5s 281us/step - loss: 0.1099 - acc: 0.9667 - val_loss: 0.4398 - val_acc: 0.8584\n",
      "Epoch 150/200\n",
      "18000/18000 [==============================] - 5s 277us/step - loss: 0.0445 - acc: 0.9933 - val_loss: 0.3981 - val_acc: 0.8705\n",
      "Epoch 151/200\n",
      "18000/18000 [==============================] - 6s 313us/step - loss: 0.0270 - acc: 0.9990 - val_loss: 0.3800 - val_acc: 0.8732\n",
      "Epoch 152/200\n",
      "18000/18000 [==============================] - 5s 292us/step - loss: 0.0236 - acc: 0.9993 - val_loss: 0.3760 - val_acc: 0.8769\n",
      "Epoch 153/200\n",
      "18000/18000 [==============================] - 6s 327us/step - loss: 0.0219 - acc: 0.9994 - val_loss: 0.3822 - val_acc: 0.8755\n",
      "Epoch 154/200\n",
      "18000/18000 [==============================] - 6s 327us/step - loss: 0.0209 - acc: 0.9994 - val_loss: 0.3829 - val_acc: 0.8759\n",
      "Epoch 155/200\n",
      "18000/18000 [==============================] - 5s 292us/step - loss: 0.0201 - acc: 0.9995 - val_loss: 0.3836 - val_acc: 0.8779\n",
      "Epoch 156/200\n",
      "18000/18000 [==============================] - 5s 282us/step - loss: 0.0192 - acc: 0.9996 - val_loss: 0.3853 - val_acc: 0.8764\n",
      "Epoch 157/200\n",
      "18000/18000 [==============================] - 5s 293us/step - loss: 0.0187 - acc: 0.9996 - val_loss: 0.3848 - val_acc: 0.8774\n",
      "Epoch 158/200\n",
      "18000/18000 [==============================] - 5s 284us/step - loss: 0.0181 - acc: 0.9997 - val_loss: 0.3906 - val_acc: 0.8774\n",
      "Epoch 159/200\n",
      "18000/18000 [==============================] - 5s 285us/step - loss: 0.0177 - acc: 0.9997 - val_loss: 0.3910 - val_acc: 0.8749\n",
      "Epoch 160/200\n",
      "18000/18000 [==============================] - 5s 299us/step - loss: 0.0177 - acc: 0.9996 - val_loss: 0.3946 - val_acc: 0.8779\n",
      "Epoch 161/200\n",
      "18000/18000 [==============================] - 5s 303us/step - loss: 0.0169 - acc: 0.9997 - val_loss: 0.4061 - val_acc: 0.8764\n",
      "Epoch 162/200\n",
      "18000/18000 [==============================] - 5s 291us/step - loss: 0.0188 - acc: 0.9990 - val_loss: 0.4442 - val_acc: 0.8644\n",
      "Epoch 163/200\n",
      "18000/18000 [==============================] - 5s 295us/step - loss: 0.1975 - acc: 0.9337 - val_loss: 0.5147 - val_acc: 0.8415\n",
      "Epoch 164/200\n",
      "18000/18000 [==============================] - 5s 299us/step - loss: 0.0658 - acc: 0.9814 - val_loss: 0.4267 - val_acc: 0.8655\n",
      "Epoch 165/200\n",
      "18000/18000 [==============================] - 5s 289us/step - loss: 0.0294 - acc: 0.9972 - val_loss: 0.3904 - val_acc: 0.8780\n",
      "Epoch 166/200\n",
      "18000/18000 [==============================] - 6s 312us/step - loss: 0.0182 - acc: 0.9995 - val_loss: 0.3826 - val_acc: 0.8788\n",
      "Epoch 167/200\n",
      "18000/18000 [==============================] - 5s 298us/step - loss: 0.0155 - acc: 0.9998 - val_loss: 0.3801 - val_acc: 0.8822\n",
      "Epoch 168/200\n",
      "18000/18000 [==============================] - 5s 293us/step - loss: 0.0143 - acc: 0.9998 - val_loss: 0.3819 - val_acc: 0.8806\n",
      "Epoch 169/200\n",
      "18000/18000 [==============================] - 5s 282us/step - loss: 0.0136 - acc: 0.9999 - val_loss: 0.3834 - val_acc: 0.8810\n",
      "Epoch 170/200\n",
      "18000/18000 [==============================] - 5s 285us/step - loss: 0.0130 - acc: 0.9999 - val_loss: 0.3872 - val_acc: 0.8813\n",
      "Epoch 171/200\n",
      "18000/18000 [==============================] - 5s 301us/step - loss: 0.0124 - acc: 0.9999 - val_loss: 0.3848 - val_acc: 0.8809\n",
      "Epoch 172/200\n",
      "18000/18000 [==============================] - 6s 316us/step - loss: 0.0120 - acc: 0.9999 - val_loss: 0.3885 - val_acc: 0.8812\n",
      "Epoch 173/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000/18000 [==============================] - 5s 298us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.3922 - val_acc: 0.8811\n",
      "Epoch 174/200\n",
      "18000/18000 [==============================] - 5s 293us/step - loss: 0.0112 - acc: 0.9999 - val_loss: 0.3920 - val_acc: 0.8803\n",
      "Epoch 175/200\n",
      "18000/18000 [==============================] - 5s 298us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.3967 - val_acc: 0.8806\n",
      "Epoch 176/200\n",
      "18000/18000 [==============================] - 5s 280us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.4004 - val_acc: 0.8796\n",
      "Epoch 177/200\n",
      "18000/18000 [==============================] - 5s 279us/step - loss: 0.0104 - acc: 0.9999 - val_loss: 0.4007 - val_acc: 0.8796\n",
      "Epoch 178/200\n",
      "18000/18000 [==============================] - 5s 279us/step - loss: 0.1333 - acc: 0.9597 - val_loss: 0.6359 - val_acc: 0.8181\n",
      "Epoch 179/200\n",
      "18000/18000 [==============================] - 5s 288us/step - loss: 0.1514 - acc: 0.9486 - val_loss: 0.4778 - val_acc: 0.8585\n",
      "Epoch 180/200\n",
      "18000/18000 [==============================] - 5s 279us/step - loss: 0.0477 - acc: 0.9884 - val_loss: 0.4094 - val_acc: 0.8737\n",
      "Epoch 181/200\n",
      "18000/18000 [==============================] - 5s 282us/step - loss: 0.0209 - acc: 0.9984 - val_loss: 0.3767 - val_acc: 0.8806\n",
      "Epoch 182/200\n",
      "18000/18000 [==============================] - 5s 304us/step - loss: 0.0130 - acc: 0.9999 - val_loss: 0.3789 - val_acc: 0.8839\n",
      "Epoch 183/200\n",
      "18000/18000 [==============================] - 5s 277us/step - loss: 0.0113 - acc: 0.9999 - val_loss: 0.3768 - val_acc: 0.8845\n",
      "Epoch 184/200\n",
      "18000/18000 [==============================] - 5s 274us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 0.3800 - val_acc: 0.8823\n",
      "Epoch 185/200\n",
      "18000/18000 [==============================] - 6s 324us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.3786 - val_acc: 0.8849\n",
      "Epoch 186/200\n",
      "18000/18000 [==============================] - 5s 305us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.3831 - val_acc: 0.8836\n",
      "Epoch 187/200\n",
      "18000/18000 [==============================] - 6s 326us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.3836 - val_acc: 0.8852\n",
      "Epoch 188/200\n",
      "18000/18000 [==============================] - 6s 317us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.3865 - val_acc: 0.8842\n",
      "Epoch 189/200\n",
      "18000/18000 [==============================] - 5s 299us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.3880 - val_acc: 0.8848\n",
      "Epoch 190/200\n",
      "18000/18000 [==============================] - 6s 358us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.3894 - val_acc: 0.8851\n",
      "Epoch 191/200\n",
      "18000/18000 [==============================] - 6s 307us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.3901 - val_acc: 0.8848\n",
      "Epoch 192/200\n",
      "18000/18000 [==============================] - 5s 300us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.3938 - val_acc: 0.8832\n",
      "Epoch 193/200\n",
      "18000/18000 [==============================] - 6s 333us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.3971 - val_acc: 0.8832 1s - loss\n",
      "Epoch 194/200\n",
      "18000/18000 [==============================] - 6s 318us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.3988 - val_acc: 0.8834\n",
      "Epoch 195/200\n",
      "18000/18000 [==============================] - 5s 279us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.3992 - val_acc: 0.8848\n",
      "Epoch 196/200\n",
      "18000/18000 [==============================] - 7s 397us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.4045 - val_acc: 0.8835\n",
      "Epoch 197/200\n",
      "18000/18000 [==============================] - 6s 353us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.4113 - val_acc: 0.8817\n",
      "Epoch 198/200\n",
      "18000/18000 [==============================] - 6s 332us/step - loss: 0.0941 - acc: 0.9754 - val_loss: 0.9756 - val_acc: 0.7665\n",
      "Epoch 199/200\n",
      "18000/18000 [==============================] - 5s 305us/step - loss: 0.2150 - acc: 0.9284 - val_loss: 0.5093 - val_acc: 0.8547\n",
      "Epoch 200/200\n",
      "18000/18000 [==============================] - 5s 281us/step - loss: 0.0433 - acc: 0.9887 - val_loss: 0.4134 - val_acc: 0.8743\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "training = model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=200,\n",
    "              validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'3 digits add/sub')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_acc = training.history['val_acc']\n",
    "acc = training.history['acc']\n",
    "plt.figure()\n",
    "plt.plot(val_acc, label = 'val_acc')\n",
    "plt.plot(acc, label = 'acc')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('3 digits add/sub')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSG : Prediction\n",
      "2  Q 967+1   T 968  \u001b[91m\u001b[0m 969 \n",
      "3  Q 932-170 T 762  \u001b[91m\u001b[0m 752 \n",
      "12  Q 437+411 T 848  \u001b[91m\u001b[0m 859 \n",
      "13  Q 40+42   T 82   \u001b[91m\u001b[0m 81  \n",
      "16  Q 960+84  T 1044 \u001b[91m\u001b[0m 1034\n",
      "19  Q 377+312 T 689  \u001b[91m\u001b[0m 698 \n",
      "23  Q 240+11  T 251  \u001b[91m\u001b[0m 261 \n",
      "29  Q 708-8   T 700  \u001b[91m\u001b[0m 709 \n",
      "30  Q 922-13  T 909  \u001b[91m\u001b[0m 919 \n",
      "33  Q 336+62  T 398  \u001b[91m\u001b[0m 388 \n",
      "35  Q 3+741   T 744  \u001b[91m\u001b[0m 745 \n",
      "36  Q 616-499 T 117  \u001b[91m\u001b[0m 128 \n",
      "39  Q 919-95  T 824  \u001b[91m\u001b[0m 823 \n",
      "40  Q 523-94  T 429  \u001b[91m\u001b[0m 439 \n",
      "41  Q 4+90    T 94   \u001b[91m\u001b[0m 96  \n",
      "46  Q 14+136  T 150  \u001b[91m\u001b[0m 149 \n",
      "47  Q 592-35  T 557  \u001b[91m\u001b[0m 547 \n",
      "48  Q 150-38  T 112  \u001b[91m\u001b[0m 122 \n",
      "52  Q 187+11  T 198  \u001b[91m\u001b[0m 290 \n",
      "53  Q 657+319 T 976  \u001b[91m\u001b[0m 986 \n",
      "54  Q 597-51  T 546  \u001b[91m\u001b[0m 536 \n",
      "56  Q 69+481  T 550  \u001b[91m\u001b[0m 540 \n",
      "58  Q 707+13  T 720  \u001b[91m\u001b[0m 710 \n",
      "59  Q 924-247 T 677  \u001b[91m\u001b[0m 676 \n",
      "62  Q 956-87  T 869  \u001b[91m\u001b[0m 879 \n",
      "63  Q 105+817 T 922  \u001b[91m\u001b[0m 939 \n",
      "65  Q 534-320 T 214  \u001b[91m\u001b[0m 204 \n",
      "67  Q 594+280 T 874  \u001b[91m\u001b[0m 875 \n",
      "68  Q 689+250 T 939  \u001b[91m\u001b[0m 928 \n",
      "73  Q 762-744 T 18   \u001b[91m\u001b[0m 9   \n",
      "77  Q 52+711  T 763  \u001b[91m\u001b[0m 773 \n",
      "82  Q 830+988 T 1818 \u001b[91m\u001b[0m 1827\n",
      "86  Q 345+56  T 401  \u001b[91m\u001b[0m 301 \n",
      "87  Q 164+63  T 227  \u001b[91m\u001b[0m 226 \n",
      "89  Q 886+23  T 909  \u001b[91m\u001b[0m 900 \n",
      "91  Q 305-163 T 142  \u001b[91m\u001b[0m 132 \n",
      "92  Q 982-221 T 761  \u001b[91m\u001b[0m 751 \n",
      "93  Q 570+406 T 976  \u001b[91m\u001b[0m 986 \n",
      "95  Q 932+682 T 1614 \u001b[91m\u001b[0m 1604\n",
      "102  Q 279-99  T 180  \u001b[91m\u001b[0m 170 \n",
      "103  Q 833+672 T 1505 \u001b[91m\u001b[0m 1504\n",
      "104  Q 612-484 T 128  \u001b[91m\u001b[0m 138 \n",
      "110  Q 956+99  T 1055 \u001b[91m\u001b[0m 1054\n",
      "122  Q 494-479 T 15   \u001b[91m\u001b[0m 14  \n",
      "125  Q 26+49   T 75   \u001b[91m\u001b[0m 85  \n",
      "126  Q 19+6    T 25   \u001b[91m\u001b[0m 24  \n",
      "128  Q 49-48   T 1    \u001b[91m\u001b[0m 0   \n",
      "129  Q 731-41  T 690  \u001b[91m\u001b[0m 680 \n",
      "131  Q 62+947  T 1009 \u001b[91m\u001b[0m 1018\n",
      "132  Q 92-81   T 11   \u001b[91m\u001b[0m 12  \n",
      "137  Q 294+678 T 972  \u001b[91m\u001b[0m 974 \n",
      "142  Q 518-335 T 183  \u001b[91m\u001b[0m 174 \n",
      "143  Q 504-54  T 450  \u001b[91m\u001b[0m 440 \n",
      "146  Q 504+22  T 526  \u001b[91m\u001b[0m 516 \n",
      "147  Q 808-1   T 807  \u001b[91m\u001b[0m 707 \n",
      "148  Q 338-48  T 290  \u001b[91m\u001b[0m 280 \n",
      "151  Q 721+892 T 1613 \u001b[91m\u001b[0m 1622\n",
      "152  Q 996-135 T 861  \u001b[91m\u001b[0m 851 \n",
      "157  Q 776+12  T 788  \u001b[91m\u001b[0m 798 \n",
      "158  Q 243-168 T 75   \u001b[91m\u001b[0m 76  \n",
      "162  Q 11+892  T 903  \u001b[91m\u001b[0m 902 \n",
      "163  Q 916+634 T 1550 \u001b[91m\u001b[0m 1659\n",
      "164  Q 941-120 T 821  \u001b[91m\u001b[0m 811 \n",
      "168  Q 13+792  T 805  \u001b[91m\u001b[0m 705 \n",
      "171  Q 30+981  T 1011 \u001b[91m\u001b[0m 1001\n",
      "173  Q 391+186 T 577  \u001b[91m\u001b[0m 588 \n",
      "175  Q 66+473  T 539  \u001b[91m\u001b[0m 549 \n",
      "176  Q 726-426 T 300  \u001b[91m\u001b[0m 200 \n",
      "179  Q 342+89  T 431  \u001b[91m\u001b[0m 421 \n",
      "180  Q 510+549 T 1059 \u001b[91m\u001b[0m 1078\n",
      "181  Q 607+7   T 614  \u001b[91m\u001b[0m 615 \n",
      "188  Q 64+2    T 66   \u001b[91m\u001b[0m 67  \n",
      "190  Q 700-689 T 11   \u001b[91m\u001b[0m 21  \n",
      "191  Q 125+418 T 543  \u001b[91m\u001b[0m 532 \n",
      "195  Q 299+962 T 1261 \u001b[91m\u001b[0m 1259\n",
      "196  Q 7+292   T 299  \u001b[91m\u001b[0m 398 \n",
      "198  Q 560+459 T 1019 \u001b[91m\u001b[0m 1009\n",
      "199  Q 15+44   T 59   \u001b[91m\u001b[0m 58  \n",
      "206  Q 364-6   T 358  \u001b[91m\u001b[0m 359 \n",
      "207  Q 237+192 T 429  \u001b[91m\u001b[0m 437 \n",
      "211  Q 51-51   T 0    \u001b[91m\u001b[0m 9   \n",
      "213  Q 960+237 T 1197 \u001b[91m\u001b[0m 1297\n",
      "215  Q 879+373 T 1252 \u001b[91m\u001b[0m 1241\n",
      "217  Q 478-126 T 352  \u001b[91m\u001b[0m 342 \n",
      "218  Q 989-4   T 985  \u001b[91m\u001b[0m 995 \n",
      "223  Q 5+57    T 62   \u001b[91m\u001b[0m 53  \n",
      "225  Q 685-609 T 76   \u001b[91m\u001b[0m 96  \n",
      "229  Q 715-320 T 395  \u001b[91m\u001b[0m 385 \n",
      "232  Q 697-305 T 392  \u001b[91m\u001b[0m 382 \n",
      "233  Q 261+20  T 281  \u001b[91m\u001b[0m 271 \n",
      "234  Q 440+40  T 480  \u001b[91m\u001b[0m 470 \n",
      "237  Q 12+787  T 799  \u001b[91m\u001b[0m 809 \n",
      "238  Q 61-23   T 38   \u001b[91m\u001b[0m 37  \n",
      "241  Q 270+847 T 1117 \u001b[91m\u001b[0m 1197\n",
      "242  Q 337-18  T 319  \u001b[91m\u001b[0m 318 \n",
      "243  Q 381-74  T 307  \u001b[91m\u001b[0m 397 \n",
      "245  Q 802-424 T 378  \u001b[91m\u001b[0m 388 \n",
      "246  Q 263+806 T 1069 \u001b[91m\u001b[0m 1080\n",
      "248  Q 96-82   T 14   \u001b[91m\u001b[0m 15  \n",
      "251  Q 385-376 T 9    \u001b[91m\u001b[0m 0   \n",
      "253  Q 390-43  T 347  \u001b[91m\u001b[0m 346 \n",
      "254  Q 594-4   T 590  \u001b[91m\u001b[0m 580 \n",
      "261  Q 201-42  T 159  \u001b[91m\u001b[0m 169 \n",
      "262  Q 955+33  T 988  \u001b[91m\u001b[0m 987 \n",
      "263  Q 995-440 T 555  \u001b[91m\u001b[0m 544 \n",
      "267  Q 259-111 T 148  \u001b[91m\u001b[0m 147 \n",
      "270  Q 560+0   T 560  \u001b[91m\u001b[0m 571 \n",
      "271  Q 49+687  T 736  \u001b[91m\u001b[0m 746 \n",
      "272  Q 605-6   T 599  \u001b[91m\u001b[0m 509 \n",
      "273  Q 891+392 T 1283 \u001b[91m\u001b[0m 1393\n",
      "274  Q 576+718 T 1294 \u001b[91m\u001b[0m 1205\n",
      "276  Q 52+168  T 220  \u001b[91m\u001b[0m 221 \n",
      "278  Q 79-16   T 63   \u001b[91m\u001b[0m 64  \n",
      "279  Q 212-15  T 197  \u001b[91m\u001b[0m 297 \n",
      "280  Q 575-540 T 35   \u001b[91m\u001b[0m 15  \n",
      "283  Q 560-61  T 499  \u001b[91m\u001b[0m 599 \n",
      "285  Q 963+263 T 1226 \u001b[91m\u001b[0m 1216\n",
      "288  Q 919-83  T 836  \u001b[91m\u001b[0m 826 \n",
      "289  Q 196+688 T 884  \u001b[91m\u001b[0m 874 \n",
      "294  Q 589+577 T 1166 \u001b[91m\u001b[0m 1174\n",
      "295  Q 261+281 T 542  \u001b[91m\u001b[0m 532 \n",
      "296  Q 614+449 T 1063 \u001b[91m\u001b[0m 1053\n",
      "305  Q 776+4   T 780  \u001b[91m\u001b[0m 770 \n",
      "307  Q 535+664 T 1199 \u001b[91m\u001b[0m 1100\n",
      "310  Q 572+42  T 614  \u001b[91m\u001b[0m 613 \n",
      "314  Q 803-721 T 82   \u001b[91m\u001b[0m 91  \n",
      "316  Q 185+737 T 922  \u001b[91m\u001b[0m 942 \n",
      "323  Q 839+954 T 1793 \u001b[91m\u001b[0m 1704\n",
      "325  Q 384+355 T 739  \u001b[91m\u001b[0m 748 \n",
      "328  Q 668+6   T 674  \u001b[91m\u001b[0m 675 \n",
      "329  Q 71+434  T 505  \u001b[91m\u001b[0m 495 \n",
      "330  Q 6+104   T 110  \u001b[91m\u001b[0m 109 \n",
      "332  Q 712+622 T 1334 \u001b[91m\u001b[0m 1344\n",
      "334  Q 898-365 T 533  \u001b[91m\u001b[0m 534 \n",
      "335  Q 736+561 T 1297 \u001b[91m\u001b[0m 1306\n",
      "337  Q 721-543 T 178  \u001b[91m\u001b[0m 188 \n",
      "338  Q 296-257 T 39   \u001b[91m\u001b[0m 11  \n",
      "339  Q 699+987 T 1686 \u001b[91m\u001b[0m 1695\n",
      "344  Q 172-42  T 130  \u001b[91m\u001b[0m 120 \n",
      "345  Q 798-692 T 106  \u001b[91m\u001b[0m 186 \n",
      "352  Q 9+401   T 410  \u001b[91m\u001b[0m 419 \n",
      "355  Q 9+288   T 297  \u001b[91m\u001b[0m 296 \n",
      "356  Q 938+365 T 1303 \u001b[91m\u001b[0m 1393\n",
      "359  Q 685-637 T 48   \u001b[91m\u001b[0m 37  \n",
      "362  Q 237+404 T 641  \u001b[91m\u001b[0m 642 \n",
      "363  Q 53+249  T 302  \u001b[91m\u001b[0m 292 \n",
      "365  Q 3+39    T 42   \u001b[91m\u001b[0m 41  \n",
      "367  Q 848+59  T 907  \u001b[91m\u001b[0m 997 \n",
      "368  Q 266+94  T 360  \u001b[91m\u001b[0m 350 \n",
      "371  Q 167+510 T 677  \u001b[91m\u001b[0m 687 \n",
      "374  Q 611+312 T 923  \u001b[91m\u001b[0m 922 \n",
      "376  Q 949-760 T 189  \u001b[91m\u001b[0m 199 \n",
      "381  Q 916-899 T 17   \u001b[91m\u001b[0m 48  \n",
      "385  Q 437+909 T 1346 \u001b[91m\u001b[0m 1356\n",
      "389  Q 83-5    T 78   \u001b[91m\u001b[0m 88  \n",
      "390  Q 876+89  T 965  \u001b[91m\u001b[0m 975 \n",
      "394  Q 33-20   T 13   \u001b[91m\u001b[0m 12  \n",
      "395  Q 451-359 T 92   \u001b[91m\u001b[0m 10  \n",
      "398  Q 855+72  T 927  \u001b[91m\u001b[0m 937 \n",
      "400  Q 455-196 T 259  \u001b[91m\u001b[0m 269 \n",
      "401  Q 890-4   T 886  \u001b[91m\u001b[0m 885 \n",
      "402  Q 3+567   T 570  \u001b[91m\u001b[0m 579 \n",
      "403  Q 506-297 T 209  \u001b[91m\u001b[0m 219 \n",
      "404  Q 212+274 T 486  \u001b[91m\u001b[0m 476 \n",
      "405  Q 817-466 T 351  \u001b[91m\u001b[0m 341 \n",
      "408  Q 948+366 T 1314 \u001b[91m\u001b[0m 1304\n",
      "409  Q 72-69   T 3    \u001b[91m\u001b[0m 4   \n",
      "412  Q 63-8    T 55   \u001b[91m\u001b[0m 56  \n",
      "413  Q 888-124 T 764  \u001b[91m\u001b[0m 763 \n",
      "414  Q 326-98  T 228  \u001b[91m\u001b[0m 229 \n",
      "418  Q 798+677 T 1475 \u001b[91m\u001b[0m 1565\n",
      "419  Q 835+305 T 1140 \u001b[91m\u001b[0m 1151\n",
      "420  Q 972-956 T 16   \u001b[91m\u001b[0m 17  \n",
      "421  Q 812-631 T 181  \u001b[91m\u001b[0m 191 \n",
      "423  Q 975+830 T 1805 \u001b[91m\u001b[0m 1807\n",
      "428  Q 689+870 T 1559 \u001b[91m\u001b[0m 1568\n",
      "429  Q 397+119 T 516  \u001b[91m\u001b[0m 515 \n",
      "430  Q 832-0   T 832  \u001b[91m\u001b[0m 822 \n",
      "434  Q 912-521 T 391  \u001b[91m\u001b[0m 491 \n",
      "436  Q 40+311  T 351  \u001b[91m\u001b[0m 361 \n",
      "440  Q 647+576 T 1223 \u001b[91m\u001b[0m 1213\n",
      "443  Q 461+848 T 1309 \u001b[91m\u001b[0m 1319\n",
      "445  Q 623-408 T 215  \u001b[91m\u001b[0m 205 \n",
      "447  Q 25+492  T 517  \u001b[91m\u001b[0m 507 \n",
      "448  Q 260-0   T 260  \u001b[91m\u001b[0m 269 \n",
      "449  Q 517+889 T 1406 \u001b[91m\u001b[0m 1486\n",
      "453  Q 36+371  T 407  \u001b[91m\u001b[0m 497 \n",
      "454  Q 325-4   T 321  \u001b[91m\u001b[0m 322 \n",
      "458  Q 325+721 T 1046 \u001b[91m\u001b[0m 1047\n",
      "461  Q 62-62   T 0    \u001b[91m\u001b[0m 1   \n",
      "462  Q 29+949  T 978  \u001b[91m\u001b[0m 988 \n",
      "467  Q 66+321  T 387  \u001b[91m\u001b[0m 388 \n",
      "470  Q 979+388 T 1367 \u001b[91m\u001b[0m 1377\n",
      "476  Q 275+974 T 1249 \u001b[91m\u001b[0m 1258\n",
      "477  Q 904-869 T 35   \u001b[91m\u001b[0m 44  \n",
      "482  Q 79+113  T 192  \u001b[91m\u001b[0m 203 \n",
      "484  Q 518+926 T 1444 \u001b[91m\u001b[0m 1434\n",
      "485  Q 574-24  T 550  \u001b[91m\u001b[0m 559 \n",
      "489  Q 343+17  T 360  \u001b[91m\u001b[0m 350 \n",
      "490  Q 79+452  T 531  \u001b[91m\u001b[0m 521 \n",
      "491  Q 492-163 T 329  \u001b[91m\u001b[0m 349 \n",
      "493  Q 728-28  T 700  \u001b[91m\u001b[0m 600 \n",
      "494  Q 722-186 T 536  \u001b[91m\u001b[0m 546 \n",
      "495  Q 381-1   T 380  \u001b[91m\u001b[0m 370 \n",
      "496  Q 107-32  T 75   \u001b[91m\u001b[0m 76  \n",
      "497  Q 20+683  T 703  \u001b[91m\u001b[0m 603 \n",
      "503  Q 345+734 T 1079 \u001b[91m\u001b[0m 1089\n",
      "506  Q 15+40   T 55   \u001b[91m\u001b[0m 54  \n",
      "510  Q 8+512   T 520  \u001b[91m\u001b[0m 529 \n",
      "511  Q 145+3   T 148  \u001b[91m\u001b[0m 158 \n",
      "513  Q 579+277 T 856  \u001b[91m\u001b[0m 855 \n",
      "516  Q 66-0    T 66   \u001b[91m\u001b[0m 64  \n",
      "523  Q 7+28    T 35   \u001b[91m\u001b[0m 45  \n",
      "524  Q 409-206 T 203  \u001b[91m\u001b[0m 202 \n",
      "531  Q 61+329  T 390  \u001b[91m\u001b[0m 381 \n",
      "532  Q 330+98  T 428  \u001b[91m\u001b[0m 418 \n",
      "533  Q 823-393 T 430  \u001b[91m\u001b[0m 420 \n",
      "535  Q 130+473 T 603  \u001b[91m\u001b[0m 693 \n",
      "537  Q 216-182 T 34   \u001b[91m\u001b[0m 33  \n",
      "539  Q 591-4   T 587  \u001b[91m\u001b[0m 586 \n",
      "542  Q 470+621 T 1091 \u001b[91m\u001b[0m 1180\n",
      "543  Q 766+752 T 1518 \u001b[91m\u001b[0m 1510\n",
      "545  Q 108+447 T 555  \u001b[91m\u001b[0m 545 \n",
      "546  Q 84+39   T 123  \u001b[91m\u001b[0m 133 \n",
      "548  Q 18+4    T 22   \u001b[91m\u001b[0m 39  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "551  Q 42-7    T 35   \u001b[91m\u001b[0m 34  \n",
      "553  Q 570-280 T 290  \u001b[91m\u001b[0m 380 \n",
      "566  Q 596-64  T 532  \u001b[91m\u001b[0m 522 \n",
      "568  Q 359-152 T 207  \u001b[91m\u001b[0m 217 \n",
      "572  Q 732-708 T 24   \u001b[91m\u001b[0m 214 \n",
      "576  Q 308-93  T 215  \u001b[91m\u001b[0m 214 \n",
      "577  Q 979+9   T 988  \u001b[91m\u001b[0m 999 \n",
      "581  Q 778-382 T 396  \u001b[91m\u001b[0m 496 \n",
      "583  Q 407-80  T 327  \u001b[91m\u001b[0m 317 \n",
      "586  Q 418-70  T 348  \u001b[91m\u001b[0m 349 \n",
      "588  Q 653-3   T 650  \u001b[91m\u001b[0m 641 \n",
      "589  Q 64+27   T 91   \u001b[91m\u001b[0m 92  \n",
      "590  Q 692-646 T 46   \u001b[91m\u001b[0m 37  \n",
      "591  Q 105-77  T 28   \u001b[91m\u001b[0m 27  \n",
      "592  Q 596-135 T 461  \u001b[91m\u001b[0m 462 \n",
      "593  Q 0+720   T 720  \u001b[91m\u001b[0m 721 \n",
      "595  Q 915+93  T 1008 \u001b[91m\u001b[0m 101 \n",
      "596  Q 348+345 T 693  \u001b[91m\u001b[0m 694 \n",
      "597  Q 649+763 T 1412 \u001b[91m\u001b[0m 1312\n",
      "599  Q 977+571 T 1548 \u001b[91m\u001b[0m 1530\n",
      "602  Q 115+25  T 140  \u001b[91m\u001b[0m 131 \n",
      "603  Q 212+551 T 763  \u001b[91m\u001b[0m 773 \n",
      "606  Q 815+155 T 970  \u001b[91m\u001b[0m 960 \n",
      "610  Q 897-95  T 802  \u001b[91m\u001b[0m 803 \n",
      "611  Q 560+950 T 1510 \u001b[91m\u001b[0m 1511\n",
      "612  Q 679-84  T 595  \u001b[91m\u001b[0m 585 \n",
      "613  Q 903+88  T 991  \u001b[91m\u001b[0m 990 \n",
      "615  Q 791-496 T 295  \u001b[91m\u001b[0m 395 \n",
      "616  Q 171-90  T 81   \u001b[91m\u001b[0m 82  \n",
      "619  Q 150-75  T 75   \u001b[91m\u001b[0m 74  \n",
      "620  Q 930-81  T 849  \u001b[91m\u001b[0m 859 \n",
      "628  Q 10+801  T 811  \u001b[91m\u001b[0m 821 \n",
      "629  Q 343+463 T 806  \u001b[91m\u001b[0m 896 \n",
      "632  Q 259-138 T 121  \u001b[91m\u001b[0m 128 \n",
      "634  Q 62-43   T 19   \u001b[91m\u001b[0m 29  \n",
      "635  Q 96-70   T 26   \u001b[91m\u001b[0m 17  \n",
      "639  Q 319+73  T 392  \u001b[91m\u001b[0m 391 \n",
      "643  Q 28+67   T 95   \u001b[91m\u001b[0m 96  \n",
      "648  Q 53+452  T 505  \u001b[91m\u001b[0m 506 \n",
      "650  Q 469+71  T 540  \u001b[91m\u001b[0m 530 \n",
      "651  Q 33+123  T 156  \u001b[91m\u001b[0m 166 \n",
      "655  Q 946+107 T 1053 \u001b[91m\u001b[0m 1063\n",
      "656  Q 830-667 T 163  \u001b[91m\u001b[0m 173 \n",
      "657  Q 23-10   T 13   \u001b[91m\u001b[0m 12  \n",
      "660  Q 659-45  T 614  \u001b[91m\u001b[0m 613 \n",
      "662  Q 758-102 T 656  \u001b[91m\u001b[0m 657 \n",
      "665  Q 658-318 T 340  \u001b[91m\u001b[0m 341 \n",
      "667  Q 601-268 T 333  \u001b[91m\u001b[0m 323 \n",
      "672  Q 339-327 T 12   \u001b[91m\u001b[0m 22  \n",
      "673  Q 60+825  T 885  \u001b[91m\u001b[0m 875 \n",
      "688  Q 703+221 T 924  \u001b[91m\u001b[0m 933 \n",
      "689  Q 903+532 T 1435 \u001b[91m\u001b[0m 1445\n",
      "691  Q 544+445 T 989  \u001b[91m\u001b[0m 998 \n",
      "695  Q 102-91  T 11   \u001b[91m\u001b[0m 10  \n",
      "696  Q 988+9   T 997  \u001b[91m\u001b[0m 100 \n",
      "701  Q 161-42  T 119  \u001b[91m\u001b[0m 129 \n",
      "706  Q 8+27    T 35   \u001b[91m\u001b[0m 44  \n",
      "708  Q 911+35  T 946  \u001b[91m\u001b[0m 945 \n",
      "709  Q 3+592   T 595  \u001b[91m\u001b[0m 695 \n",
      "711  Q 840+962 T 1802 \u001b[91m\u001b[0m 1822\n",
      "714  Q 915-37  T 878  \u001b[91m\u001b[0m 879 \n",
      "716  Q 996-39  T 957  \u001b[91m\u001b[0m 946 \n",
      "719  Q 110-86  T 24   \u001b[91m\u001b[0m 26  \n",
      "721  Q 36+80   T 116  \u001b[91m\u001b[0m 117 \n",
      "724  Q 193+736 T 929  \u001b[91m\u001b[0m 159 \n",
      "727  Q 408-82  T 326  \u001b[91m\u001b[0m 317 \n",
      "729  Q 954-7   T 947  \u001b[91m\u001b[0m 948 \n",
      "733  Q 860-694 T 166  \u001b[91m\u001b[0m 176 \n",
      "734  Q 561-449 T 112  \u001b[91m\u001b[0m 122 \n",
      "737  Q 437+411 T 848  \u001b[91m\u001b[0m 859 \n",
      "744  Q 321+135 T 456  \u001b[91m\u001b[0m 465 \n",
      "746  Q 62-33   T 29   \u001b[91m\u001b[0m 39  \n",
      "748  Q 774+754 T 1528 \u001b[91m\u001b[0m 1520\n",
      "754  Q 341+821 T 1162 \u001b[91m\u001b[0m 1252\n",
      "756  Q 219+893 T 1112 \u001b[91m\u001b[0m 1111\n",
      "757  Q 887-288 T 599  \u001b[91m\u001b[0m 509 \n",
      "759  Q 698+585 T 1283 \u001b[91m\u001b[0m 1284\n",
      "763  Q 433-351 T 82   \u001b[91m\u001b[0m 73  \n",
      "764  Q 174-104 T 70   \u001b[91m\u001b[0m 72  \n",
      "765  Q 391-165 T 226  \u001b[91m\u001b[0m 246 \n",
      "766  Q 150+211 T 361  \u001b[91m\u001b[0m 272 \n",
      "767  Q 611+40  T 651  \u001b[91m\u001b[0m 650 \n",
      "768  Q 161+143 T 304  \u001b[91m\u001b[0m 203 \n",
      "774  Q 223+597 T 820  \u001b[91m\u001b[0m 810 \n",
      "775  Q 69+274  T 343  \u001b[91m\u001b[0m 344 \n",
      "779  Q 680-20  T 660  \u001b[91m\u001b[0m 650 \n",
      "780  Q 427-1   T 426  \u001b[91m\u001b[0m 425 \n",
      "783  Q 594-269 T 325  \u001b[91m\u001b[0m 324 \n",
      "791  Q 368-159 T 209  \u001b[91m\u001b[0m 219 \n",
      "792  Q 408+84  T 492  \u001b[91m\u001b[0m 502 \n",
      "794  Q 285-91  T 194  \u001b[91m\u001b[0m 104 \n",
      "795  Q 34+9    T 43   \u001b[91m\u001b[0m 35  \n",
      "797  Q 550+958 T 1508 \u001b[91m\u001b[0m 1517\n",
      "800  Q 189+512 T 701  \u001b[91m\u001b[0m 600 \n",
      "802  Q 678+519 T 1197 \u001b[91m\u001b[0m 1187\n",
      "804  Q 491+697 T 1188 \u001b[91m\u001b[0m 1198\n",
      "805  Q 2+6     T 8    \u001b[91m\u001b[0m 08  \n",
      "807  Q 264+139 T 403  \u001b[91m\u001b[0m 494 \n",
      "811  Q 812-631 T 181  \u001b[91m\u001b[0m 191 \n",
      "812  Q 937-688 T 249  \u001b[91m\u001b[0m 259 \n",
      "815  Q 8+73    T 81   \u001b[91m\u001b[0m 82  \n",
      "818  Q 687+63  T 750  \u001b[91m\u001b[0m 751 \n",
      "823  Q 655+593 T 1248 \u001b[91m\u001b[0m 1141\n",
      "828  Q 73+790  T 863  \u001b[91m\u001b[0m 873 \n",
      "830  Q 882+970 T 1852 \u001b[91m\u001b[0m 1851\n",
      "831  Q 756-372 T 384  \u001b[91m\u001b[0m 374 \n",
      "832  Q 524-399 T 125  \u001b[91m\u001b[0m 135 \n",
      "835  Q 40+933  T 973  \u001b[91m\u001b[0m 983 \n",
      "837  Q 19+969  T 988  \u001b[91m\u001b[0m 998 \n",
      "840  Q 301+317 T 618  \u001b[91m\u001b[0m 528 \n",
      "841  Q 997+786 T 1783 \u001b[91m\u001b[0m 1773\n",
      "848  Q 84-81   T 3    \u001b[91m\u001b[0m 1   \n",
      "850  Q 963+46  T 1009 \u001b[91m\u001b[0m 1018\n",
      "852  Q 48+710  T 758  \u001b[91m\u001b[0m 768 \n",
      "856  Q 897+83  T 980  \u001b[91m\u001b[0m 981 \n",
      "857  Q 188+989 T 1177 \u001b[91m\u001b[0m 1176\n",
      "859  Q 25+92   T 117  \u001b[91m\u001b[0m 107 \n",
      "861  Q 808-442 T 366  \u001b[91m\u001b[0m 375 \n",
      "862  Q 141-41  T 100  \u001b[91m\u001b[0m 901 \n",
      "863  Q 912+415 T 1327 \u001b[91m\u001b[0m 1338\n",
      "865  Q 463-4   T 459  \u001b[91m\u001b[0m 469 \n",
      "866  Q 483+108 T 591  \u001b[91m\u001b[0m 691 \n",
      "872  Q 486-211 T 275  \u001b[91m\u001b[0m 375 \n",
      "873  Q 105+97  T 202  \u001b[91m\u001b[0m 192 \n",
      "874  Q 853+26  T 879  \u001b[91m\u001b[0m 889 \n",
      "879  Q 189-87  T 102  \u001b[91m\u001b[0m 112 \n",
      "881  Q 76+942  T 1018 \u001b[91m\u001b[0m 1017\n",
      "884  Q 552-490 T 62   \u001b[91m\u001b[0m 53  \n",
      "885  Q 248-234 T 14   \u001b[91m\u001b[0m 23  \n",
      "886  Q 603+58  T 661  \u001b[91m\u001b[0m 671 \n",
      "887  Q 588+9   T 597  \u001b[91m\u001b[0m 588 \n",
      "889  Q 496+576 T 1072 \u001b[91m\u001b[0m 1052\n",
      "894  Q 773+747 T 1520 \u001b[91m\u001b[0m 1510\n",
      "895  Q 742-27  T 715  \u001b[91m\u001b[0m 705 \n",
      "897  Q 849+491 T 1340 \u001b[91m\u001b[0m 1330\n",
      "898  Q 118+74  T 192  \u001b[91m\u001b[0m 101 \n",
      "902  Q 825-8   T 817  \u001b[91m\u001b[0m 816 \n",
      "905  Q 592-95  T 497  \u001b[91m\u001b[0m 498 \n",
      "906  Q 127-59  T 68   \u001b[91m\u001b[0m 78  \n",
      "907  Q 707-89  T 618  \u001b[91m\u001b[0m 617 \n",
      "909  Q 91-1    T 90   \u001b[91m\u001b[0m 80  \n",
      "911  Q 40+412  T 452  \u001b[91m\u001b[0m 462 \n",
      "913  Q 81+149  T 230  \u001b[91m\u001b[0m 220 \n",
      "915  Q 689-228 T 461  \u001b[91m\u001b[0m 451 \n",
      "922  Q 77+143  T 220  \u001b[91m\u001b[0m 219 \n",
      "923  Q 961-890 T 71   \u001b[91m\u001b[0m 60  \n",
      "926  Q 517+550 T 1067 \u001b[91m\u001b[0m 1076\n",
      "930  Q 752-7   T 745  \u001b[91m\u001b[0m 735 \n",
      "931  Q 546+528 T 1074 \u001b[91m\u001b[0m 1064\n",
      "932  Q 13+702  T 715  \u001b[91m\u001b[0m 725 \n",
      "933  Q 752+39  T 791  \u001b[91m\u001b[0m 881 \n",
      "936  Q 65+742  T 807  \u001b[91m\u001b[0m 707 \n",
      "937  Q 84+197  T 281  \u001b[91m\u001b[0m 271 \n",
      "939  Q 198+82  T 280  \u001b[91m\u001b[0m 271 \n",
      "942  Q 771-81  T 690  \u001b[91m\u001b[0m 680 \n",
      "948  Q 770+28  T 798  \u001b[91m\u001b[0m 898 \n",
      "949  Q 200+123 T 323  \u001b[91m\u001b[0m 322 \n",
      "955  Q 196+688 T 884  \u001b[91m\u001b[0m 874 \n",
      "956  Q 603-589 T 14   \u001b[91m\u001b[0m 22  \n",
      "957  Q 981-306 T 675  \u001b[91m\u001b[0m 676 \n",
      "958  Q 486-211 T 275  \u001b[91m\u001b[0m 375 \n",
      "960  Q 948-844 T 104  \u001b[91m\u001b[0m 18  \n",
      "962  Q 549+588 T 1137 \u001b[91m\u001b[0m 1148\n",
      "963  Q 71+708  T 779  \u001b[91m\u001b[0m 788 \n",
      "964  Q 24+999  T 1023 \u001b[91m\u001b[0m 1022\n",
      "967  Q 338-160 T 178  \u001b[91m\u001b[0m 188 \n",
      "970  Q 847+852 T 1699 \u001b[91m\u001b[0m 1709\n",
      "971  Q 958-17  T 941  \u001b[91m\u001b[0m 931 \n",
      "972  Q 622-225 T 397  \u001b[91m\u001b[0m 497 \n",
      "973  Q 61+998  T 1059 \u001b[91m\u001b[0m 1069\n",
      "975  Q 483-140 T 343  \u001b[91m\u001b[0m 332 \n",
      "976  Q 103-24  T 79   \u001b[91m\u001b[0m 89  \n",
      "977  Q 200+193 T 393  \u001b[91m\u001b[0m 303 \n",
      "978  Q 398+7   T 405  \u001b[91m\u001b[0m 404 \n",
      "979  Q 68+442  T 510  \u001b[91m\u001b[0m 501 \n",
      "980  Q 900+735 T 1635 \u001b[91m\u001b[0m 1645\n",
      "981  Q 15+10   T 25   \u001b[91m\u001b[0m 13  \n",
      "985  Q 203+128 T 331  \u001b[91m\u001b[0m 329 \n",
      "987  Q 406+93  T 499  \u001b[91m\u001b[0m 498 \n",
      "988  Q 292-21  T 271  \u001b[91m\u001b[0m 272 \n",
      "989  Q 649-524 T 125  \u001b[91m\u001b[0m 115 \n",
      "991  Q 877-65  T 812  \u001b[91m\u001b[0m 813 \n",
      "992  Q 672+629 T 1301 \u001b[91m\u001b[0m 1290\n",
      "995  Q 197-87  T 110  \u001b[91m\u001b[0m 100 \n",
      "996  Q 498-69  T 429  \u001b[91m\u001b[0m 430 \n",
      "998  Q 149+558 T 707  \u001b[91m\u001b[0m 698 \n",
      "accu: 0.5943166666666667\n"
     ]
    }
   ],
   "source": [
    "print(\"MSG : Prediction\")\n",
    "#####################################################\n",
    "## Try to test and evaluate your model ##############\n",
    "## ex. test_x = [\"555+175\", \"860+7  \", \"340+29 \"]\n",
    "## ex. test_y = [\"730 \", \"867 \", \"369 \"] \n",
    "#####################################################\n",
    "#####################################################\n",
    "correct_ct = 0\n",
    "accuracy = 0\n",
    "for i in range(len(test_x)):\n",
    "    ind = np.random.randint(0, len(test_x))\n",
    "    rowx, rowy = test_x[np.array([ind])], test_y[np.array([ind])]\n",
    "    preds = model.predict_classes(rowx, verbose=0)\n",
    "    q = ctable.decode(rowx[0])\n",
    "    correct = ctable.decode(rowy[0])\n",
    "    guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "    if correct == guess:\n",
    "        correct_ct = correct_ct + 1\n",
    "    else: \n",
    "        if i < 1000:\n",
    "            print(i, ' Q', q[::-1] if REVERSE else q, end=' ')\n",
    "            print('T', correct, end=' ')\n",
    "            print(colors.fail + '' + colors.close, end=' ')\n",
    "            print(guess)\n",
    "accuracy = correct_ct/len(test_x)\n",
    "print('accu:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accu: 0.5943166666666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
